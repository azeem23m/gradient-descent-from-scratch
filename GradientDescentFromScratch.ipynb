{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d8bd42b-8cce-4580-b9c3-01a4c320dfb7",
   "metadata": {},
   "source": [
    "# Gradient Descent Variants From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3c2ba8-2e1d-4636-8181-de3fa9d57da8",
   "metadata": {},
   "source": [
    "## First we implement some helper classes\n",
    "DataLoader class: Handle splitting data into batches. \n",
    "It takes X, y, number of batches, shuffle.\n",
    "It has get_batches() function that split the data, shuffle and generate batches efficiently by using a Generator.\n",
    "\n",
    "LinearRegression class: Simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "893a40fd-b5cd-4e05-96ee-b62387a6755c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ca1f424-00f5-4918-acbf-ae585316d083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, X, y, batch_size=32, shuffle=True):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "\n",
    "    def get_data(self):\n",
    "        indices = list(range(self.X.shape[0]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "        for i in indices:\n",
    "            yield self.X[i], self.y[i]\n",
    "\n",
    "    def get_random_sample(self):\n",
    "        rand_i = np.random.randint(self.X.shape[0])\n",
    "        return self.X[rand_i], self.y[rand_i]\n",
    "    \n",
    "    def get_batches(self):\n",
    "        indices = list(range(self.X.shape[0]))\n",
    "        if self.shuffle:\n",
    "            np.random.shuffle(indices)\n",
    "        # Batching Data\n",
    "        for i in range(0, len(indices), self.batch_size):\n",
    "            batch_indices = indices[i:i+self.batch_size]\n",
    "            yield self.X[batch_indices], self.y[batch_indices]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10bfc54c-0367-4bdd-9d76-333863ec6ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self, n_inputs):\n",
    "        self.n_inputs = n_inputs\n",
    "        self.w = np.zeros((n_inputs, 1))\n",
    "        self.b = np.zeros(1)\n",
    "\n",
    "    def params(self):\n",
    "        return self.w, self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return  X @ self.w + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca3c9dd-ce46-45c0-a669-706e512897b7",
   "metadata": {},
   "source": [
    "# Batch Gradient Descent #\n",
    "\n",
    "It's an optimization algorithm that is used to find optimal parameters $\\theta$ that minimizes the objective function $L(\\theta)$.\n",
    "\n",
    "Gradient is vector of all partial derivatives of a function where this vector is the direction of the steepest ascent. We take a step in the opposite direction of it (to minimize loss) and scale by a learning rate $\\eta$.\n",
    "\n",
    "In batch gradient descent we use all of the data to compute gradient.\n",
    "\n",
    "$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm:} \\text{Gradient Descent} \\\\\n",
    "\\hline\n",
    "\\textbf{Input:} \\text{ Learning rate } \\eta, \\text{ initial parameters } \\theta, \\text{ objective function } L(\\theta), \\text{number of epoches} \\\\\n",
    "\\textbf{Output:} \\text{Optimized parameters } \\theta^* \\\\\n",
    "1. \\quad \\text{Initialize } \\theta \\\\\n",
    "2. \\quad \\text{Repeat for number of epoches or until convergence:} \\\\\n",
    "3. \\quad \\quad \\text{Compute gradient } \\nabla L(\\theta) \\\\\n",
    "4. \\quad \\quad \\text{Update parameters: } \\theta \\leftarrow \\theta - \\eta L(\\theta) \\\\\n",
    "5. \\quad \\text{Return optimized } \\theta^* \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc2edad-56ef-4fcc-8e2c-6d8e1775a4b6",
   "metadata": {},
   "source": [
    "The objective function is:\n",
    "$$\n",
    "    L(\\theta) = \\frac{1}{2m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)^2\n",
    "$$\n",
    "where $ \\hat{y} = Xw + b $.\n",
    "\n",
    "Gradient with respect to parameters $ w $ using the chain rule:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i) x_i\n",
    "$$\n",
    "In vector notation:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial w} = \\frac{1}{m} X^T (Xw + b - y)\n",
    "$$\n",
    "\n",
    "Gradient with respect to the bias term $ b $:\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^{m} (\\hat{y}_i - y_i)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e19f3cf2-fa85-4f83-b4e4-764d89e975ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGD:\n",
    "    def __init__(self, model, X, y, lr, epochs):\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        \n",
    "    def loss(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2) / 2\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        w, b = self.model.params()\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        \n",
    "        grad_w = (1 / m) * X.T @ errors\n",
    "        grad_b = (1 / m) * np.sum(errors)\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def step(self, grad):\n",
    "        grad_w, grad_b = grad\n",
    "        w, b = self.model.params()\n",
    "        w -= self.lr * grad_w\n",
    "        b -= self.lr * grad_b\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.model.predict(self.X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss(self.y, y_pred)\n",
    "            \n",
    "            # Calculate Gradient\n",
    "            grad = self.gradient(self.X, self.y)\n",
    "            \n",
    "            # Step (Update Params)\n",
    "            self.step(grad)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{epoch}/{self.epochs} ------ Loss:{loss}\")\n",
    "                \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        loss = self.loss(y_test, y_pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a9a4a6-86de-4213-a6f7-e36445bd185f",
   "metadata": {},
   "source": [
    "## Now lets test it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94dd9408-1051-4425-9832-6bd5514f3914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [0. 0. 0. 0. 0. 0. 0.]\n",
      "Initial bias: [0.]\n",
      "Training the model...\n",
      "0/500 ------ Loss:9.084076333040409\n",
      "10/500 ------ Loss:7.428025105079007\n",
      "20/500 ------ Loss:6.075156658817498\n",
      "30/500 ------ Loss:4.9697315820571\n",
      "40/500 ------ Loss:4.066300916943799\n",
      "50/500 ------ Loss:3.327798475172903\n",
      "60/500 ------ Loss:2.723988263206627\n",
      "70/500 ------ Loss:2.2302006452103074\n",
      "80/500 ------ Loss:1.8263033269085505\n",
      "90/500 ------ Loss:1.4958633523044294\n",
      "100/500 ------ Loss:1.2254645110955615\n",
      "110/500 ------ Loss:1.0041512172682547\n",
      "120/500 ------ Loss:0.8229753301363458\n",
      "130/500 ------ Loss:0.6746267841645981\n",
      "140/500 ------ Loss:0.5531324647051017\n",
      "150/500 ------ Loss:0.45361066848720016\n",
      "160/500 ------ Loss:0.372070846204997\n",
      "170/500 ------ Loss:0.3052502419376963\n",
      "180/500 ------ Loss:0.2504806032481112\n",
      "190/500 ------ Loss:0.20557940384953474\n",
      "200/500 ------ Loss:0.16876105226654847\n",
      "210/500 ------ Loss:0.13856439923078648\n",
      "220/500 ------ Loss:0.11379353961099747\n",
      "230/500 ------ Loss:0.09346946068560467\n",
      "240/500 ------ Loss:0.0767905412518402\n",
      "250/500 ------ Loss:0.06310027470622322\n",
      "260/500 ------ Loss:0.05186088949339555\n",
      "270/500 ------ Loss:0.04263178493869316\n",
      "280/500 ------ Loss:0.03505189980780775\n",
      "290/500 ------ Loss:0.028825293393785336\n",
      "300/500 ------ Loss:0.023709351365345962\n",
      "310/500 ------ Loss:0.019505136592068928\n",
      "320/500 ------ Loss:0.016049493224468435\n",
      "330/500 ------ Loss:0.013208584139731293\n",
      "340/500 ------ Loss:0.010872600470161076\n",
      "350/500 ------ Loss:0.008951429756460176\n",
      "360/500 ------ Loss:0.007371108303253269\n",
      "370/500 ------ Loss:0.006070915181786745\n",
      "380/500 ------ Loss:0.005000991346051862\n",
      "390/500 ------ Loss:0.004120388580557437\n",
      "400/500 ------ Loss:0.00339547035834715\n",
      "410/500 ------ Loss:0.002798600872143141\n",
      "420/500 ------ Loss:0.002307070093150097\n",
      "430/500 ------ Loss:0.0019022121868882795\n",
      "440/500 ------ Loss:0.0015686823615966758\n",
      "450/500 ------ Loss:0.0012938635589310405\n",
      "460/500 ------ Loss:0.0010673795773174768\n",
      "470/500 ------ Loss:0.0008806954563434881\n",
      "480/500 ------ Loss:0.0007267894181937446\n",
      "490/500 ------ Loss:0.0005998834999756372\n"
     ]
    }
   ],
   "source": [
    "# Generate some data to test it\n",
    "np.random.seed(42)\n",
    "X = np.random.randn(1000, 7)\n",
    "true_w = np.array([[2.0], [1.0], [0.5], [-1.0], [1.5], [-0.8], [0.3]])\n",
    "true_b = 3.0\n",
    "y = X @ true_w + true_b\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression(n_inputs=7)\n",
    "optimizerGD = BGD(\n",
    "    model=model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "print(\"Initial weights:\", model.w.flatten())\n",
    "print(\"Initial bias:\", model.b.flatten())\n",
    "\n",
    "print(\"Training the model...\")\n",
    "optimizerGD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bbbf50f-eded-4eba-9246-cbdc358d247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Loss: 0.0004997497837414081\n",
      "Learned weights: [ 1.98723973  0.99662395  0.4961774  -0.99778266  1.48517637 -0.79651228\n",
      "  0.30465994]\n",
      "Learned bias: [2.97558819]\n",
      "True weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "True bias: 3.0\n"
     ]
    }
   ],
   "source": [
    "loss = optimizerGD.evaluate(X_test, y_test)\n",
    "print(\"\\nFinal Loss:\", loss)\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"Learned weights:\", model.w.flatten())\n",
    "print(\"Learned bias:\", model.b.flatten())\n",
    "print(\"True weights:\", true_w.flatten())\n",
    "print(\"True bias:\", true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3649469f-6598-455e-a763-139966a5d289",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent\n",
    "\n",
    "Same as gradient descent but instead of computing the gradient using all of the training data in each epoch, we use a random sample in each one.\n",
    "DataLoader class will handle this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fe3fce8-d60e-45ab-b221-37e4159271a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    def __init__(self, model, X, y, lr, epochs):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.dl = DataLoader(X, y, batch_size=0, shuffle=True)\n",
    "        \n",
    "    def loss(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2) / 2\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        w, b = self.model.params()\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        grad_w = (1 / m) * X.reshape(-1, 1) @ errors\n",
    "        grad_b = (1 / m) * np.sum(errors)\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def step(self, grad):\n",
    "        grad_w, grad_b = grad\n",
    "        w, b = self.model.params()\n",
    "\n",
    "        w -= self.lr * grad_w.reshape(-1, 1)\n",
    "        b -= self.lr * grad_b\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.model.predict(self.dl.X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss(self.dl.y, y_pred)\n",
    "            \n",
    "            # Calculate Gradient\n",
    "            sample_X, sample_y = self.dl.get_random_sample()\n",
    "            grad = self.gradient(sample_X, sample_y)\n",
    "            \n",
    "            # Step (Update Params)\n",
    "            self.step(grad)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{epoch}/{self.epochs} ------ Loss:{loss}\")\n",
    "                \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        loss = self.loss(y_test, y_pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef75f595-ce58-416f-b5e9-f23ee401b8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [ 1.98723973  0.99662395  0.4961774  -0.99778266  1.48517637 -0.79651228\n",
      "  0.30465994]\n",
      "Initial bias: [2.97558819]\n",
      "Training the model...\n",
      "0/500 ------ Loss:9.084076333040409\n",
      "10/500 ------ Loss:7.546580196433368\n",
      "20/500 ------ Loss:6.44018419153675\n",
      "30/500 ------ Loss:5.623505924228909\n",
      "40/500 ------ Loss:4.6316845295947715\n",
      "50/500 ------ Loss:4.306092072656013\n",
      "60/500 ------ Loss:3.750043385481879\n",
      "70/500 ------ Loss:2.9734068225508143\n",
      "80/500 ------ Loss:2.213266589066577\n",
      "90/500 ------ Loss:1.9454591325695223\n",
      "100/500 ------ Loss:1.7712665574041162\n",
      "110/500 ------ Loss:1.5572975403421272\n",
      "120/500 ------ Loss:1.1488232712178688\n",
      "130/500 ------ Loss:0.9947401428762147\n",
      "140/500 ------ Loss:0.8237668988130463\n",
      "150/500 ------ Loss:0.7066872888227677\n",
      "160/500 ------ Loss:0.5583453512519764\n",
      "170/500 ------ Loss:0.4791856737800803\n",
      "180/500 ------ Loss:0.4243203029910982\n",
      "190/500 ------ Loss:0.33287578841758136\n",
      "200/500 ------ Loss:0.28025348698771996\n",
      "210/500 ------ Loss:0.24291569684092423\n",
      "220/500 ------ Loss:0.17449804696217205\n",
      "230/500 ------ Loss:0.12338385083156853\n",
      "240/500 ------ Loss:0.09830647266400537\n",
      "250/500 ------ Loss:0.07622955729910175\n",
      "260/500 ------ Loss:0.059204695721975925\n",
      "270/500 ------ Loss:0.04766508306901153\n",
      "280/500 ------ Loss:0.03824390359366528\n",
      "290/500 ------ Loss:0.02729146631229278\n",
      "300/500 ------ Loss:0.02192773252267812\n",
      "310/500 ------ Loss:0.01860306619122692\n",
      "320/500 ------ Loss:0.01414368907707394\n",
      "330/500 ------ Loss:0.01122179030583154\n",
      "340/500 ------ Loss:0.009073635792398678\n",
      "350/500 ------ Loss:0.006967016470563053\n",
      "360/500 ------ Loss:0.00522987596222566\n",
      "370/500 ------ Loss:0.004372347762218306\n",
      "380/500 ------ Loss:0.0033829129016693742\n",
      "390/500 ------ Loss:0.002939869224982597\n",
      "400/500 ------ Loss:0.0025189126557735504\n",
      "410/500 ------ Loss:0.0023112492467231346\n",
      "420/500 ------ Loss:0.0017080045958921377\n",
      "430/500 ------ Loss:0.0014559461565827384\n",
      "440/500 ------ Loss:0.0012723509595138412\n",
      "450/500 ------ Loss:0.000991261107131142\n",
      "460/500 ------ Loss:0.000744348410679903\n",
      "470/500 ------ Loss:0.0005816806369675826\n",
      "480/500 ------ Loss:0.00048723871489072987\n",
      "490/500 ------ Loss:0.00039114378847269734\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial weights:\", model.w.flatten()) # Same initial weights\n",
    "print(\"Initial bias:\", model.b.flatten())\n",
    "\n",
    "model_2 = LinearRegression(n_inputs=7) # Initialize model again to reset parameters\n",
    "optimizerSGD = SGD(\n",
    "    model=model_2,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "print(\"Training the model...\")\n",
    "optimizerSGD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "42137363-921b-4d52-8dbd-d2a64e75b15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Loss: 0.000326817971718045\n",
      "Learned weights: [ 1.98981612  0.9942291   0.50267473 -0.99495884  1.49444201 -0.80401952\n",
      "  0.29786237]\n",
      "Learned bias: [2.97957605]\n",
      "True weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "True bias: 3.0\n"
     ]
    }
   ],
   "source": [
    "loss_2 = optimizerSGD.evaluate(X_test, y_test)\n",
    "print(\"\\nFinal Loss:\", loss_2)\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"Learned weights:\", model_2.w.flatten())\n",
    "print(\"Learned bias:\", model_2.b.flatten())\n",
    "print(\"True weights:\", true_w.flatten())\n",
    "print(\"True bias:\", true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b47465-e9d5-4c3a-bc5c-14b92bd31b84",
   "metadata": {},
   "source": [
    "## Mini-Batch Gradient Descent\n",
    "Now instead of using entire training data at once, we split it into small batches. We can introduce randomness by shuffling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3bad8917-82e6-47b7-a44d-69d6960260a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MBGD:\n",
    "    def __init__(self, model, X, y, lr, batch_size, shuffle, epochs):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.shuffle = shuffle\n",
    "        self.epochs = epochs\n",
    "        self.dl = DataLoader(X, y, batch_size, shuffle)\n",
    "        \n",
    "    def loss(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2) / 2\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        w, b = self.model.params()\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        \n",
    "        grad_w = (1 / m) * X.T @ errors\n",
    "        grad_b = (1 / m) * np.sum(errors)\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def step(self, grad):\n",
    "        grad_w, grad_b = grad\n",
    "        w, b = self.model.params()\n",
    "        w -= self.lr * grad_w\n",
    "        b -= self.lr * grad_b\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.model.predict(self.dl.X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss(self.dl.y, y_pred)\n",
    "            \n",
    "            for batch_X, batch_y in self.dl.get_batches():\n",
    "                # Calculate Gradient\n",
    "                grad = self.gradient(batch_X, batch_y)\n",
    "            \n",
    "                # Step (Update Params)\n",
    "                self.step(grad)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{epoch}/{self.epochs} ------ Loss:{loss}\")\n",
    "                \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        loss = self.loss(y_test, y_pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6aa00077-b164-4394-90e2-ab1f7df480b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [ 1.98723973  0.99662395  0.4961774  -0.99778266  1.48517637 -0.79651228\n",
      "  0.30465994]\n",
      "Initial bias: [2.97558819]\n",
      "Training the model...\n",
      "0/500 ------ Loss:9.084076333040409\n",
      "10/500 ------ Loss:0.015799170452084593\n",
      "20/500 ------ Loss:3.5388478622233407e-05\n",
      "30/500 ------ Loss:9.293554512849136e-08\n",
      "40/500 ------ Loss:2.7355031985888855e-10\n",
      "50/500 ------ Loss:8.710171132838194e-13\n",
      "60/500 ------ Loss:2.9389255319659832e-15\n",
      "70/500 ------ Loss:1.0806239964752252e-17\n",
      "80/500 ------ Loss:3.969720948568418e-20\n",
      "90/500 ------ Loss:1.4967839401916643e-22\n",
      "100/500 ------ Loss:5.5525702411967775e-25\n",
      "110/500 ------ Loss:2.1685291027788554e-27\n",
      "120/500 ------ Loss:1.592320421050237e-28\n",
      "130/500 ------ Loss:1.436362866607076e-28\n",
      "140/500 ------ Loss:1.3914569595773698e-28\n",
      "150/500 ------ Loss:1.3837076337787377e-28\n",
      "160/500 ------ Loss:1.361299793246902e-28\n",
      "170/500 ------ Loss:1.3470297925095198e-28\n",
      "180/500 ------ Loss:1.3479961471184155e-28\n",
      "190/500 ------ Loss:1.345201853880703e-28\n",
      "200/500 ------ Loss:1.350415731426148e-28\n",
      "210/500 ------ Loss:1.3437146045553284e-28\n",
      "220/500 ------ Loss:1.3499547408346595e-28\n",
      "230/500 ------ Loss:1.3440644150629873e-28\n",
      "240/500 ------ Loss:1.3388562074552986e-28\n",
      "250/500 ------ Loss:1.340299822911853e-28\n",
      "260/500 ------ Loss:1.3375107065738309e-28\n",
      "270/500 ------ Loss:1.3395033199166127e-28\n",
      "280/500 ------ Loss:1.3423752666496829e-28\n",
      "290/500 ------ Loss:1.3376857350871768e-28\n",
      "300/500 ------ Loss:1.3390068305843892e-28\n",
      "310/500 ------ Loss:1.3371347650486866e-28\n",
      "320/500 ------ Loss:1.3384235665525914e-28\n",
      "330/500 ------ Loss:1.3394197499644659e-28\n",
      "340/500 ------ Loss:1.340202201374832e-28\n",
      "350/500 ------ Loss:1.3358097252469482e-28\n",
      "360/500 ------ Loss:1.3358676572196752e-28\n",
      "370/500 ------ Loss:1.3393201562751816e-28\n",
      "380/500 ------ Loss:1.3405303182075974e-28\n",
      "390/500 ------ Loss:1.343573842187553e-28\n",
      "400/500 ------ Loss:1.344743821517609e-28\n",
      "410/500 ------ Loss:1.3387684466795926e-28\n",
      "420/500 ------ Loss:1.34058701758516e-28\n",
      "430/500 ------ Loss:1.341234376565507e-28\n",
      "440/500 ------ Loss:1.344416937280008e-28\n",
      "450/500 ------ Loss:1.3420165814568402e-28\n",
      "460/500 ------ Loss:1.3422569375138998e-28\n",
      "470/500 ------ Loss:1.344194084074283e-28\n",
      "480/500 ------ Loss:1.3366414804638905e-28\n",
      "490/500 ------ Loss:1.3408498068742117e-28\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial weights:\", model.w.flatten()) # Same initial weights\n",
    "print(\"Initial bias:\", model.b.flatten())\n",
    "\n",
    "model_3 = LinearRegression(n_inputs=7) # Initialize model again to reset parameters\n",
    "optimizerMBGD = MBGD(\n",
    "    model=model_3,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "print(\"Training the model...\")\n",
    "optimizerMBGD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04dc8d44-6e93-4cc8-aeb5-fbe39b27bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Loss: 1.3644397061687145e-28\n",
      "Learned weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "Learned bias: [3.]\n",
      "True weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "True bias: 3.0\n"
     ]
    }
   ],
   "source": [
    "loss_3 = optimizerMBGD.evaluate(X_test, y_test)\n",
    "print(\"\\nFinal Loss:\", loss_3)\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"Learned weights:\", model_3.w.flatten())\n",
    "print(\"Learned bias:\", model_3.b.flatten())\n",
    "print(\"True weights:\", true_w.flatten())\n",
    "print(\"True bias:\", true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c38dae9-5ddb-4929-aa99-eb430b797b65",
   "metadata": {},
   "source": [
    "## Momentum Based Gradient Descent\n",
    "It helps to speed up training by smoothing out updates and reducing oscillations. It remembers past gradients, giving the optimizer \"momentum\" to move faster and avoid getting stuck in narrow valleys.\n",
    "The update rules:\n",
    "$$\n",
    "\\text{Velocity update:} \\quad v_{t} = \\beta v_{t-1} + \\eta \\nabla_{\\theta} L(\\theta_{t-1})\n",
    "$$\n",
    "$$\n",
    "\\text{Parameter update:} \\quad \\theta_{t} = \\theta_{t-1} - v_{t}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "$v_t$ : Velocity at time step $t$.\n",
    "\n",
    "$\\beta$: Momentum hyperparameter.\n",
    "\n",
    "$\\eta$: Learning rate.\n",
    "\n",
    "$\\nabla_{\\theta} L(\\theta_t)$: Gradient of the cost function $\\theta$ with respect to parameters $\\theta$ at time step $t$.\n",
    "\n",
    "$\\theta_t$: Model parameters at time step $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4195c12a-16b9-4fd6-bdc7-2dfeadc0a2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MGD:\n",
    "    def __init__(self, model, X, y, lr, batch_size, shuffle, epochs, momentum=0.9):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.shuffle = shuffle\n",
    "        self.epochs = epochs\n",
    "        self.momentum = momentum\n",
    "        self.dl = DataLoader(X, y, batch_size, shuffle)\n",
    "        \n",
    "        # Initialize velocity terms to zero\n",
    "        w, b = self.model.params()\n",
    "        self.v_w = np.zeros_like(w)\n",
    "        self.v_b = np.zeros_like(b)\n",
    "        \n",
    "    def loss(self, y, y_hat):\n",
    "        return np.mean((y - y_hat)**2) / 2\n",
    "    \n",
    "    def gradient(self, X, y):\n",
    "        m = len(y)\n",
    "        w, b = self.model.params()\n",
    "        predictions = X @ w + b\n",
    "        errors = predictions - y\n",
    "        \n",
    "        grad_w = ((1 / m) * X.T @ errors).reshape(-1, 1)\n",
    "        grad_b = (1 / m) * np.sum(errors)\n",
    "        \n",
    "        return grad_w, grad_b\n",
    "    \n",
    "    def step(self, grad):\n",
    "        grad_w, grad_b = grad\n",
    "        \n",
    "        # Update velocity\n",
    "        self.v_w = self.momentum * self.v_w + self.lr * grad_w\n",
    "        self.v_b = self.momentum * self.v_b + self.lr * grad_b\n",
    "        \n",
    "        # Update parameters\n",
    "        self.model.w -= self.v_w\n",
    "        self.model.b -= self.v_b\n",
    "        \n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            y_pred = self.model.predict(self.dl.X)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = self.loss(self.dl.y, y_pred)\n",
    "            \n",
    "            for batch_X, batch_y in self.dl.get_batches():\n",
    "                # Calculate Gradient\n",
    "                grad = self.gradient(batch_X, batch_y)\n",
    "            \n",
    "                # Step (Update Params)\n",
    "                self.step(grad)\n",
    "            \n",
    "            if epoch % 10 == 0:\n",
    "                print(f\"{epoch}/{self.epochs} ------ Loss:{loss}\")\n",
    "                \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        loss = self.loss(y_test, y_pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "448c289a-ace1-4f77-9832-280c0ed4c3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights: [ 1.98723973  0.99662395  0.4961774  -0.99778266  1.48517637 -0.79651228\n",
      "  0.30465994]\n",
      "Initial bias: [2.97558819]\n",
      "Training the model...\n",
      "0/500 ------ Loss:9.084076333040409\n",
      "10/500 ------ Loss:3.2319091793310056e-14\n",
      "20/500 ------ Loss:2.0379467138144223e-28\n",
      "30/500 ------ Loss:8.884792464084526e-31\n",
      "40/500 ------ Loss:8.884792464084526e-31\n",
      "50/500 ------ Loss:8.884792464084526e-31\n",
      "60/500 ------ Loss:8.884792464084526e-31\n",
      "70/500 ------ Loss:8.884792464084526e-31\n",
      "80/500 ------ Loss:8.884792464084526e-31\n",
      "90/500 ------ Loss:8.884792464084526e-31\n",
      "100/500 ------ Loss:8.884792464084526e-31\n",
      "110/500 ------ Loss:8.884792464084526e-31\n",
      "120/500 ------ Loss:8.884792464084526e-31\n",
      "130/500 ------ Loss:8.884792464084526e-31\n",
      "140/500 ------ Loss:8.884792464084526e-31\n",
      "150/500 ------ Loss:8.884792464084526e-31\n",
      "160/500 ------ Loss:8.884792464084526e-31\n",
      "170/500 ------ Loss:8.884792464084526e-31\n",
      "180/500 ------ Loss:8.884792464084526e-31\n",
      "190/500 ------ Loss:8.884792464084526e-31\n",
      "200/500 ------ Loss:8.884792464084526e-31\n",
      "210/500 ------ Loss:8.884792464084526e-31\n",
      "220/500 ------ Loss:8.884792464084526e-31\n",
      "230/500 ------ Loss:8.884792464084526e-31\n",
      "240/500 ------ Loss:8.884792464084526e-31\n",
      "250/500 ------ Loss:8.884792464084526e-31\n",
      "260/500 ------ Loss:8.884792464084526e-31\n",
      "270/500 ------ Loss:8.884792464084526e-31\n",
      "280/500 ------ Loss:8.884792464084526e-31\n",
      "290/500 ------ Loss:8.884792464084526e-31\n",
      "300/500 ------ Loss:8.884792464084526e-31\n",
      "310/500 ------ Loss:8.884792464084526e-31\n",
      "320/500 ------ Loss:8.884792464084526e-31\n",
      "330/500 ------ Loss:8.884792464084526e-31\n",
      "340/500 ------ Loss:8.884792464084526e-31\n",
      "350/500 ------ Loss:8.884792464084526e-31\n",
      "360/500 ------ Loss:8.884792464084526e-31\n",
      "370/500 ------ Loss:8.884792464084526e-31\n",
      "380/500 ------ Loss:8.884792464084526e-31\n",
      "390/500 ------ Loss:8.884792464084526e-31\n",
      "400/500 ------ Loss:8.884792464084526e-31\n",
      "410/500 ------ Loss:8.884792464084526e-31\n",
      "420/500 ------ Loss:8.884792464084526e-31\n",
      "430/500 ------ Loss:8.884792464084526e-31\n",
      "440/500 ------ Loss:8.884792464084526e-31\n",
      "450/500 ------ Loss:8.884792464084526e-31\n",
      "460/500 ------ Loss:8.884792464084526e-31\n",
      "470/500 ------ Loss:8.884792464084526e-31\n",
      "480/500 ------ Loss:8.884792464084526e-31\n",
      "490/500 ------ Loss:8.884792464084526e-31\n"
     ]
    }
   ],
   "source": [
    "print(\"Initial weights:\", model.w.flatten()) # Same initial weights\n",
    "print(\"Initial bias:\", model.b.flatten())\n",
    "\n",
    "model_4 = LinearRegression(n_inputs=7) # Initialize model again to reset parameters\n",
    "optimizerMGD = MGD(\n",
    "    model=model_4,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    lr=0.01,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    epochs=500\n",
    ")\n",
    "\n",
    "print(\"Training the model...\")\n",
    "optimizerMGD.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83e0f14a-e8f5-4a83-a7fb-9e82adf80bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Loss: 1.3644397061687145e-28\n",
      "Learned weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "Learned bias: [3.]\n",
      "True weights: [ 2.   1.   0.5 -1.   1.5 -0.8  0.3]\n",
      "True bias: 3.0\n"
     ]
    }
   ],
   "source": [
    "loss_4 = optimizerMBGD.evaluate(X_test, y_test)\n",
    "print(\"\\nFinal Loss:\", loss_4)\n",
    "\n",
    "# Print learned parameters\n",
    "print(\"Learned weights:\", model_4.w.flatten())\n",
    "print(\"Learned bias:\", model_4.b.flatten())\n",
    "print(\"True weights:\", true_w.flatten())\n",
    "print(\"True bias:\", true_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14e941d-2d50-401d-936f-115a755c3dbb",
   "metadata": {},
   "source": [
    "# Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99b6a37a-41ff-41c9-9822-7d7899001423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Algorithm</th>\n",
       "      <td>Batch GD</td>\n",
       "      <td>Stochastic GD</td>\n",
       "      <td>Mini-Batch GD</td>\n",
       "      <td>Momentum GD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss</th>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.000327</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0              1              2            3\n",
       "Algorithm  Batch GD  Stochastic GD  Mini-Batch GD  Momentum GD\n",
       "Loss         0.0005       0.000327            0.0          0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(\n",
    "    {\n",
    "        'Algorithm':['Batch GD', 'Stochastic GD', 'Mini-Batch GD', 'Momentum GD'],\n",
    "    'Loss':[loss, loss_2, loss_3, loss_4]\n",
    "    }\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7ad3ed-98d5-48e0-b1b5-01e07cd2ed0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
